{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check od-mstar is successfully installed\n",
    "from od_mstar3 import cpp_mstar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple world\n",
    "num_agents = 1\n",
    "simple_agent = \\\n",
    "         [[ 1,  0,  0,  0,  0,  2, 0],\n",
    "          [ 0,  0,  0,  0,  0,  0, 0],\n",
    "          [ 0,  0,  0,  0,  0,  0, 0],\n",
    "          [ 0,  0,  0,  0,  0,  0, 0],\n",
    "          [ 0,  0,  0,  0,  0,  0, 0],\n",
    "          [ 0,  0,  0,  3,  0,  0, 0]]\n",
    "simple_world = \\\n",
    "         [[  0,  0,  0,  0,  0,  0, 0],\n",
    "          [  0,  0,  0,  1,  0,  0, 0],\n",
    "          [  1,  0,  0,  0,  1,  0, 0],\n",
    "          [  0,  0,  0,  1,  0,  0, 0],\n",
    "          [  0,  0,  0,  0,  0,  0, 0],\n",
    "          [  0,  0,  0,  0,  0,  0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warehouse_env.warehouse_env import WarehouseEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (5, 5), 1: (4, 0), 2: (2, 2)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleEnv = WarehouseEnv(agent_map=np.array(simple_agent), \n",
    "             obstacle_map=np.array(simple_world))\n",
    "simpleEnv.assign_goal(agent=0, goal=(5, 5))\n",
    "simpleEnv.assign_goal(agent=1, goal=(4, 0))\n",
    "simpleEnv.assign_goal(agent=2, goal=(2, 2))\n",
    "simpleEnv.agent_goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6, 7), (6, 7))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleEnv.obstacle_map.shape, simpleEnv.agent_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "def get_torch_screen(env, agent_id=0):\n",
    "    render_array = np.asarray(env.render(agent_id=agent_id))[:,:,:3]\n",
    "    screen = render_array.transpose((2, 0, 1))\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "def get_torch_observation(env, agent):\n",
    "#     screen = np.array(env._observe(agent)).transpose((2, 0, 1))\n",
    "#     screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    torch_tensor = torch.from_numpy(env._observe(agent))\n",
    "    return torch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleEnv._observe(agent=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6, 7])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_torch_observation(simpleEnv, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANIAAAC0CAYAAADhNHIFAAAC2ElEQVR4nO3dwWnbYBiAYblkii6QOWRoslLpoYceulMpSHNkgazh3oPBRH3RrzjPc7fkQ14+ISv6Tpen6TINsH5fRpyWncyP5yHnXV/G/F19GXJWuDNCgoCQICAkCAgJAkKCgJAgICQICAkCQoKAkCAgJAgICQJCgoCQICAkCAgJAkKCgJAgICQICAkCQoKAkCAgJAgICQJCgoCQICAkCAgJAqdlWYZso/hszucx2xmWxdaPPZhIEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgSEBAEhQUBIEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgSEBAEhQUBIEBASBIQEASFBQEgQOE3TNGQbhS0J+7AFYx8mEgSEBAEhQUBIEHgY/QWOYJ7n9HjruqbH4/hMJAgICQIu7d74Mf3c9Lnf06/4m/CRmEgQEBIEhAQBIUFASBAQEgSEBAEhQUBIEBASBIQEASFBQEgQ8PT3G57iZgsTCQJCgoBLu8k7Fvh/JhIEhAQBIUFASBB4+GxbA0axFeK+mUgQEBIEhAQBIUFASBAQEgSEBAEhQUBIEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgSEBAEhQUBIEBASBIQEASFBQEgQEBIEhAQBIUFASBCw+nIntkLcNxMJAkKCgJAgICQIXL3ZMM9zepJ1XdPjwdGYSBAQEgRu/470etp25K+XbZ+DD8hEgoCQICAkCAgJAkKCgJAgICQICAkCQoKAkCAgJAgICQJCgsDtp789xQ03mUgQEBIErl7aeccCvI+JBAEhQUBIEBASBGyj2MnfecwNnG9r+7JPrjORICAkCAgJAkKCgJsN7Oaet5yYSBAQEgRc2jHG88YtJ3+O+f9xJhIEhAQBIUFASBAQEgSEBAEhQUBIEBASBIQEASFBQEgQEBIEPP3NGAd9insrEwkCQoKASzt2c6R3LNRMJAgICQJCgoCQIPAPRlw5M/n4/igAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=210x180 at 0x7FE14AE3A710>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envRender = simpleEnv.render(zoom_size=30, agent_id=None)\n",
    "envRender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANIAAAC0CAYAAADhNHIFAAACsklEQVR4nO3dwWnjUBRAUWlIFZkSpgq78O8mMiXMLNKEsg4EAspF35bP2dsShssT9sdv3f4t2zLD7ylXXW5jzLkwp/Zr9g3AGQgJAkKCgJAgICQICAkCQoKAkCAgJAgICQJCgoCQICAkCAgJAkKCgJAgICQICAkCQoKAkCAgJAgICQJCgoCQICAkCAgJAkKCgJAgICQIrGOMOdsonsz1ep1y3WH7xiFMJAgICQJCgoCQICAkCAgJAkKCgJAgICQICAkCQoKAkCAgJAgICQJCgoCQICAkCAgJAkKCgJAgICQICAkCQoKAkCAgJAgICQJCgoCQICAkCKzLskzZRmFLwjFswTiGiQQBIUFASBAQEgSEBAEhQUBIEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgSEBAEhQUBIEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgSEBIGXZ9saMIutEOdmIkFASBAQEgSEBAEhQUBIEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgSEBAEhQUBIEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgSEBAEhQeBl9g08C1shzs1EgoCQICAkCAgJAl9+2XC5XNKL3G639P3Owud8HiYSBIQEgW9/R1r/7nvj7c++1z2tdd33um1r74NdTCQICAkCQoKAkCAgJAgICQJCgoCQICAkCAgJAkKCgJAgICQIfHv62ynugzjF/dBMJAgICQLrGMMzBfyQiQQBIUFASBAQEgTW5W3ODxjj3Z8Zch4mEgSEBAEhQUBIELir1Ze2M/CoTCQICAkCd/Vo98n/ndsZXp3B5XgmEgSEBAEhQUBIEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgTu9/S3U9w8EBMJAkKCwF092vmPBR6ViQQBIUFASBAQEgQ+AFc3OIV4zhISAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=210x180 at 0x7FE14AEE8518>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envRender = simpleEnv.render(zoom_size=30, agent_id=1)\n",
    "envRender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANIAAAC0CAYAAADhNHIFAAACx0lEQVR4nO3dwW3bQBBAUTJwFW4gdVCFc+twAXEbyt0wIJj54DLSe3eRPvhjCHnNWe9/lvsywfjYZ9yWk2y325T7jn3O79WvKXeFJyMkCAgJAkKCgJAgICQICAkCQoKAkCAgJAgICQJCgoCQICAkCAgJAkKCgJAgICQICAkCQoKAkCAgJAgICQJCgoCQICAkCAgJAkKCgJAgsO77PmUbxau5TdrOsE/azvBqTCQICAkCQoKAkCAgJAgICQJCgoCQICAkCAgJAkKCgJAgICQICAkCQoKAkCAgJAgICQJCgoCQICAkCAgJAkKCgJAgICQICAkCQoKAkCAgJAisy7JM2UZhS8I5bME4h4kEASFBQEgQEBIE3mb/AFewbVt6vTFGej2uz0SCgJAg4NHuq8/12OfeLYd/ZSYSBIQEASFBQEgQEBIEhAQBIUFASBAQEgSEBAEhQUBIEBASBJz+/sopbg4wkSAgJAh4tFu8Y4F/ZyJBQEgQEBIEhASBt1fbGjCLrRDPzUSCgJAgICQICAkCQoKAkCAgJAgICQJCgoCQICAkCAgJAkKCgJAgICQICAkCQoKAkCAgJAgICQJCgoCQICAkCAgJAkKCgJAgICQICAkCQoKA1ZcnsRXiuZlIEBASBIQEASFB4NsvG7ZtS28yxkivB1djIkFASBB4/HekdT125fv92OfgP2QiQUBIEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgSEBAEhQeDx6W+nuOEhEwkCQoLAt4923rEAP2MiQUBIEBASBIQEAdsoTrL9vk257/iwBeMMJhIEhAQBIUFASBDwZQOneeYtJyYSBIQEAY92zPF5cMvJ+zX/P85EgoCQICAkCAgJAkKCgJAgICQICAkCQoKAkCAgJAgICQJCgoDT38xx0VPcR5lIEBASBDzacZorvWOhZiJBQEgQEBIEhASBvx8GOoonovvWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=210x180 at 0x7FE14AE98D30>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envRender = simpleEnv.render(zoom_size=30, agent_id=1, other_agents_same=True)\n",
    "envRender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 40, 46])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_torch_screen(simpleEnv).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "plt.figure()\n",
    "plt.imshow(simpleEnv.render(zoom_size=30, agent_id=None),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = simpleEnv\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# PEHUEN's HYPERPARAMTERS\n",
    "NUM_STEPS = 100\n",
    "NUM_AGENTS = 3\n",
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "init_screen = get_torch_screen(env)\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "def select_optimal_action(state):\n",
    "    with torch.no_grad():\n",
    "        # t.max(1) will return largest column value of each row.\n",
    "        # second column on max result is index of where max element was\n",
    "        # found, so we pick action with the larger expected reward.\n",
    "        return policy_net(state).max(1)[1].view(1, 1)\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-b41b7a566ac1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Select and perform an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_agent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mcurrent_agent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcurrent_agent_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mNUM_AGENTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-9b3a4ed7ab4c>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# second column on max result is index of where max element was\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# found, so we pick action with the larger expected reward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/primal2/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-0c485fd74282>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/primal2/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/primal2/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             self.weight, self.bias, bn_training, exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/primal2/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2056\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   2057\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2058\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2059\u001b[0m     )\n\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 10000\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    for agent_id in env.agent_goal.keys():\n",
    "            env.assign_goal(agent_id, env.get_new_goal_location())\n",
    "    current_agent_id = 0\n",
    "    \n",
    "    last_screen = get_torch_screen(env=env, agent_id=current_agent_id)\n",
    "    current_screen = get_torch_screen(env=env, agent_id=current_agent_id)\n",
    "    state = current_screen # - last_screen\n",
    "    for t in np.arange(NUM_STEPS):\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(current_agent_id, action.item())\n",
    "        current_agent_id = (current_agent_id + 1) % NUM_AGENTS\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_torch_screen(env=env, agent_id=current_agent_id)\n",
    "        if not done:\n",
    "            next_state = current_screen #- last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the env\n",
    "observation = env.reset()\n",
    "for agent_id in env.agent_goal.keys():\n",
    "    env.assign_goal(agent_id, env.get_new_goal_location())\n",
    "frames = []\n",
    "current_agent_id = 0\n",
    "for t in range(100):\n",
    "    #Render to frames buffer\n",
    "    frames.append(simpleEnv.render(zoom_size=30, agent_id=None))\n",
    "    action = select_optimal_action(get_torch_screen(env=env, agent_id=current_agent_id))\n",
    "    _, _, done, _ = simpleEnv.step(current_agent_id, action)\n",
    "    current_agent_id = (current_agent_id + 1) % NUM_AGENTS\n",
    "    if done:\n",
    "        break\n",
    "env.close()\n",
    "frames[0].save('gym_animation.gif',\n",
    "               save_all=True, append_images=frames[1:], optimize=False, duration=100, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANIAAAC0CAYAAADhNHIFAAACwklEQVR4nO3dwY3TQBiA0THaKmiAOpyyOHAAaZtCsuugAdoIJy6rlRY5nzIkfu8ejy+fficZeZZt264DuMmn2TcAz0BIEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgSEBAEhQUBIEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgSEBAEhQUBIEBASBIQEASFB4GX2DZzF5XKZsu62bVPWPRsTCQJCgoCQICAkCAgJAkKCgJAgICQICAkCQoKAkCAgJAgICQJCgoCQICAkCAgJAkKCgJAgICQICAkCQoKAkCAgJAgICQJCgoCQICAkCAgJAssY4zpjYack3IdTMO7DRIKAkCAgJAgICQKOvhxjrOuaXm/f9/R6/P9MJAgICQIe7d74Or4d+tzr+B7fCY/ERIKAkCAgJAgICQJCgoCQICAkCAgJAkKCgJAgICQICAkCQoKA3d9v2MXNESYSBIQEAY92wzsWuJ2JBAEhQUBIEBASBF7OdmrALE6FeG4mEgSEBAEhQUBIEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgSEBAEhQUBIEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgSEBAEhQUBIEBASBBx9eSdOhXhuJhIEhAQBIUFASBAQEgSEBAEhQUBIEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgSEBAEhQUBIEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgROdxrF+uUyZd3917lOo/i57lPWfV1+TFnXRIKAkCAgJAgICQLv/tiwrmu6yL7/2xfPWevCrUwkCAgJAh//j/R7OXblz9djn/trObju9cZ14QATCQJCgoCQICAkCAgJAkKCgJAgICQICAkCQoKAkCAgJAgICQIf7/6+dRf3UXZx80BMJAgICQLvPtrNeteBdyzwqEwkCAgJAkKCgJAg8Ac5njUAZlJMcwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=210x180 at 0x7FF7108E9320>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.render(zoom_size=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(zoom_size=30, agent_id=None)\n",
    "action = select_optimal_action(get_torch_screen(env=env, agent_id=0))\n",
    "_, _, done, _ = env.step(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2]])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_optimal_action(get_torch_screen(env=env, agent_id=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: (0, 0), 1: (0, 6), 2: (5, 0)},\n",
       " array([[1, 0, 0, 0, 0, 2, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 3, 0, 0, 0]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0]]),\n",
       " tensor([[2]]))"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.agent_state, env.agent_map, env.goal_map, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primal2",
   "language": "python",
   "name": "primal2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
